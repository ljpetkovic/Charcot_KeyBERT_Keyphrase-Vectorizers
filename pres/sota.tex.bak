\begin{frame}{Définitions de la tâche}
\centering
Extraction de \og{}phrases-clés\fg{} (angl. \textit{keyphrases})
\begin{itemize}
\item séquences de plusieurs mots (ex. \textit{sclérose latérale amyotrophique})
\item reflètent plus précisément le contexte sémantique du texte \\\small{$\neq$ mots-clés : unigrammes de mot, ex. \textit{sclérose}}
\end{itemize}
%\begin{block}{Extraction de phrases-clés}
%\justifying
%Processus de \underline{sélection} automatique d'un petit ensemble de phrases les plus pertinentes à partir d'un texte donné \citep{schopf2022}.
%\end{block}
%\begin{block}{Prédiction de phrases-clés}
%\justifying
%Processus de \underline{génération} des phrases-clés qui résument parfaitement un document donné \citep{xie2023}.
\bigskip
\begin{columns}[t,onlytextwidth]
\column{.45\textwidth}
\textcolor{violet}{Extraction}\hfill
\justifying
\small
\underline{Sélection} \\
d'un ensemble de phrases les plus pertinentes à partir d'un texte.
\begin{flushright}
\small{\citep{schopf2022}}
\end{flushright} 
\column{.45\textwidth}
\textcolor{violet}{Prédiction}\hfill
\justifying
\small
\underline{Génération} \\
des phrases-clés qui résument parfaitement un document donné.
\begin{flushright}
\small{\citep{xie2023}}
\end{flushright}
\end{columns}
\end{frame}

\begin{frame}{Approches classiques\\ {\small(\hypersetup{citecolor=yellow}\cite{garaudclassiques})}}
\setbeamercolor{block title}{use=structure,fg=red!50!black,bg=yellow!40}
\setbeamercolor{block body}{use=structure,fg=black,bg=white!20!white}
\begin{block}{\textsc{Statistiques}}
\justifying
Basées sur les fréquences des mots / groupe de mots et leur cooccurrence.
\end{block}
\begin{itemize}
\small
\item \textsc{TF-IDF} -- \textit{Term Frequency $\cdot$ Inverse Document Frequency} \hfill \citep{sparck1972statistical}
\item \textsc{RAKE} -- \textit{Rapid Automatic Keyword Extraction} \hfill \citep{rose2010automatic}
\item \textsc{YAKE} -- \textit{Yet Another Keyword Extractor} \hfill \citep{CAMPOS2020257}
\end{itemize}

\begin{block}{\textsc{Graphes}}
\justifying
Chaque nœud = mot / groupe de mots ; \\chaque arc = probabilité (ou la fréquence) d’observer ces mots ensemble.
\end{block}
\begin{itemize}
\small
\item SingleRank \hfill \citep{wan2008}
\item TextRank \hfill \citep{mihalcea2004}
\item TopicRank \hfill \citep{bougouin2013topicrank}
\end{itemize}

\end{frame}

\begin{frame}{Approches sémantiques\\ {\small(\hypersetup{citecolor=yellow}\cite{garaudsemantiques})}}
\justifying
%Lier des mots sémantiquement proches et en extraire ceux qui apportent l’information la plus pertinente à l'aide des réseaux de neurones.
\setbeamercolor{block body}{use=structure,fg=black,bg=white!20!white}
\begin{block}{\textsc{Plongements de mots}}
\justifying
Représentent l’ensemble des mots d’un vocabulaire sous forme de vecteurs. Distance entre ces vecteurs $\rightarrow$ mots sémantiquement proches.
%\begin{itemize}
%\item représentation vectorielle de l’ensemble des mots d’un vocabulaire 
%\item distance entre ces vecteurs $\rightarrow$ mots sémantiquement proches
%\end{itemize}
\end{block}
\begin{itemize}
\item \small{\texttt{fastTextRank}\footnote{\url{https://github.com/jeekim/fasttextrank}}}
\end{itemize}

%\colorbox{yellow!40}{\color{red!50!black}{Plongements de mots}}
%%\textsc{\textit{word2vec}}} \small{\citep{mikolov2013efficient}}}
%\begin{itemize}
%\small
%%\item réseaux de neurones artificiels \og{}simples\fg{}
%\item représentation vectorielle de l’ensemble des mots d’un vocabulaire
%\item distance entre ces vecteurs $\rightarrow$ mots sémantiquement proches
%\item ex. : \texttt{fastTextRank}\footnote{\url{https://github.com/jeekim/fasttextrank}}
%\end{itemize}
\medskip

\setbeamercolor{block body}{use=structure,fg=black,bg=white!20!white}
\begin{block}{\textsc{Plongements contextuels}}
\justifying
Basés sur les modèles de langue pré-entraînés.\\
Gèrent mieux des cas ambiguës (homographes).
%\begin{itemize}
%\item basés sur les modèles de langue pré-entraînés 
%\item gestion des cas ambiguës (homographes)
%\end{itemize}
\end{block}
\begin{itemize}
\item \small{Key2Vec \hfill \citep{mahata2018key2vec}
\item Key\textsc{BERT} \hfill \small{\citep{grootendorst2020keybert}}}
\end{itemize}
%\colorbox{yellow!40}{\color{red!50!black}{Plongements contextuels}}
%\begin{itemize}
%\small
%%\item réseaux de neurones artificiels \og{}profonds\fg{} (angl. \textit{deep learning})
%\item basés sur les modèles de langue pré-entraînés
%\item prise en compte du contexte pour mieux capturer la sémantique du texte
%%\item architecture gourmande en temps de calcul et en volume de données
%\item ex. : Key2Vec \citep{mahata2018key2vec}, Key\textsc{BERT} \small{\citep{grootendorst2020keybert}}
%\end{itemize}
\end{frame}

%\begin{frame}{Approches exploitant les modèles de langue -- \texttt{keybert}}
%\justifying
%Librairie Python qui exploite les plongements \textsc{BERT} et la similarité cosinus pour générer les mots/phrases-clés les plus similaires à un document.
%\end{frame}
