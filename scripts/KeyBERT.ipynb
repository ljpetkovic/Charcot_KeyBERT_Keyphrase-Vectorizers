{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljpetkovic/Charcot_KeyBERT_Keyphrase-Vectorizers/blob/main/scripts/KeyBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction des mots/phrases-clés avec `keybert` et `keyphrase-vectorizers`\n",
        "---"
      ],
      "metadata": {
        "id": "9tWVEeLgzpck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJz6QJUR4upy"
      },
      "source": [
        "# `keybert`\n",
        "* librairie Python pour extraire des mots/phrases-clés les plus similaires à un document en exploitant les plongements BERT<br>\n",
        "⚠️ on doit spécifier la longueur des n-grammes à extraire, alors que l'on ne sait pas quelle est la longueur optimale<br>\n",
        "⚠️ la grammaticalité des phrases n'est pas prise en compte\n",
        "\n",
        "<p align=\"right\"><a href=\"https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db\">Schopf, 2022</a></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keybert\n",
        "\n",
        "import torch # print(torch.__version__)\n",
        "import os\n",
        "from google.colab import drive\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keybert import KeyBERT\n",
        "\n",
        "\"\"\" Initialiser le modèle de phrase :\n",
        "en l'occurrence, le modèle dérive\n",
        "les plongements de phrases sémantiquement signifiants\n",
        "qui peuvent être comparées en utilisant\n",
        "la similarité cosinus.\n",
        "\"\"\"\n",
        "sentence_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
        "kw_model = KeyBERT(model=sentence_model)\n",
        "\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir les chemins vers les fichiers d'entrée et de sortie\n",
        "path = '/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/corpus/'\n",
        "file_name = 'echantillon_charcot.txt'\n",
        "file_path = '../output/test_keybert.txt'\n",
        "\n",
        "\"\"\" Extraction des mots-clés\n",
        "1) Si on divise le texte en tranches\n",
        "\"\"\"\n",
        "with open(os.path.join(path, file_name), 'r') as myfile, open(os.path.join(path, file_path), 'w') as outfile:\n",
        "    raw_data = myfile.readlines()\n",
        "    start = 0\n",
        "    end = 200 # diminuer le nb de lignes à traiter si la mémoire RAM est épuisée\n",
        "    while len(raw_data) >= end:\n",
        "        data = \" \".join(raw_data[start:end])\n",
        "        start = end\n",
        "        end += 200\n",
        "        keywords = kw_model.extract_keywords(data, keyphrase_ngram_range=(1, 3), stop_words=None, use_mmr=True, diversity=0.7)\n",
        "        for k in keywords:\n",
        "            print(k)\n",
        "            # Writing each keyword to the output file\n",
        "            outfile.write(str(k) + '\\n')\n",
        "\n",
        "\"\"\" 2) Si on passe le fichier entier \"\"\"\n",
        "\n",
        "# with open(os.path.join(path, file_name), 'r') as myfile, open(os.path.join(path, file_path), 'w') as outfile:\n",
        "#     data = myfile.readlines(10000) # pour s'arrêter après 10 000 premiers caractères\n",
        "#     data = [line.strip('\\n') for line in data]\n",
        "#     data = ' '.join(data)\n",
        "#     # print(data)\n",
        "#     keywords = kw_model.extract_keywords(data, keyphrase_ngram_range=(1, 3), stop_words=None, use_mmr=True, diversity=0.7)\n",
        "#     for k in keywords:\n",
        "#         print(k)\n",
        "#         # Writing each keyword to the output file\n",
        "#         outfile.write(str(k) + '\\n')"
      ],
      "metadata": {
        "id": "jze7Ppsn7cKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deeac55d-5c4c-438a-b767-f5696b346ec7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "('moëlle épinière 45', 0.3381)\n",
            "('topographie anatomo pathologique', 0.2841)\n",
            "('paul auteur date', 0.2187)\n",
            "('texte explicatif mais', 0.1587)\n",
            "('du système', -0.1271)\n",
            "('scientifique les planches', 0.4944)\n",
            "('chambre photographique horizontale', 0.1779)\n",
            "('objectif de appareil', 0.1028)\n",
            "('et nous lui', 0.086)\n",
            "('parfaire cette édition', 0.0582)\n",
            "('antérieure corne postérieure', 0.486)\n",
            "('les cellules sont', 0.2331)\n",
            "('ii canal central', 0.1677)\n",
            "('16 et', 0.0078)\n",
            "('fourni cet auteur', 0.0044)\n",
            "('postérieure cordon postérieur', 0.5078)\n",
            "('ces pièces proviennent', 0.0734)\n",
            "('25 et', 0.0328)\n",
            "('méthode de weigert', -0.0611)\n",
            "('extrêmement intenses seules', -0.0653)\n",
            "('avoisinante cellule ganglionnaire', 0.3836)\n",
            "('la préparation précédente', 0.2169)\n",
            "('rosés figure 30', 0.1267)\n",
            "('voit la diminution', 0.1194)\n",
            "('même sujet planche', 0.0422)\n",
            "('faisceau pyramidal intact', 0.3281)\n",
            "('38', 0.1808)\n",
            "('secondaire descendante région', 0.1495)\n",
            "('en du tissu', 0.1241)\n",
            "('préparation montre un', 0.1076)\n",
            "('lcucocythcs substance granuleuse', 0.3474)\n",
            "('deux foyers', 0.1446)\n",
            "('55 57 et', 0.0391)\n",
            "('cette préparation montre', -0.0016)\n",
            "('par la méthode', -0.0174)\n",
            "('cervicale la figure', 0.4311)\n",
            "('antérieur partie fondamentale', 0.1289)\n",
            "('ici ces préparations', 0.072)\n",
            "('privées de prolongements', 0.0282)\n",
            "('où elle', 0.0158)\n",
            "('cervico dorsale', 0.465)\n",
            "('méthode de pall', 0.0929)\n",
            "('130 où on', 0.0397)\n",
            "('moyenne et colorée', 0.0361)\n",
            "('publié en collaboration', -0.0258)\n",
            "('faisceau postérieur tumeur', 0.4276)\n",
            "('complètement détruite', 0.334)\n",
            "('par les cordons', 0.105)\n",
            "('20 août 1885', 0.0485)\n",
            "('leçon 491 observation', 0.0036)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PatternRank\n",
        "* `keybert` + `keyphrase-vectorizers` = PatternRank<br>\n",
        " ❇️ pas besoin de spécifier la longueur des n-grammes à extraire, car la librairie l'infère elle-même<br>\n",
        "❇️ la grammaticalité des phrases est prise en compte grâce aux extractions des parties du discours (p. ex. `<N.*>*<ADJ.*>*<ADJ.*>+`--> _sclérose latérale amyotrophique_)\n",
        "* _cf._ [Schopf _et al._, 2022](https://arxiv.org/pdf/2210.05245.pdf) et [Schopf, 2022](https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhJMTOxTfUnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keyphrase-vectorizers\n",
        "# !pip install keybert\n",
        "# !pip install flair\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from keybert import KeyBERT\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import os\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ajuster les chemins\n",
        "path = '/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/corpus/'\n",
        "input_file_name = 'test.txt'\n",
        "output_file_name = '../output/test_output.txt'\n",
        "\n",
        "# Initialiser le modèle KeyBERT multilingue\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings('google-bert/bert-base-multilingual-cased'))\n",
        "\n",
        "# Ajuster les paramètres\n",
        "vectorizer = KeyphraseCountVectorizer(spacy_pipeline='fr_core_news_lg', pos_pattern='<N.*>+<ADJ.*>*', stop_words='french')\n",
        "\n",
        "with open(os.path.join(path, input_file_name), 'r') as input_file, \\\n",
        "     open(os.path.join(path, output_file_name), 'w') as output_file:\n",
        "    raw_data = input_file.readlines()\n",
        "    start = 0\n",
        "    end = 22 # diviser le texte en tranches\n",
        "    while start < len(raw_data):  # s'assurer que l'on traite toutes les données\n",
        "        data = \" \".join(raw_data[start:end]).replace('\\n', ' ')  # Joindre les lignes and gérer les nouvelles lignes\n",
        "        start = end\n",
        "        end += 22\n",
        "        try:\n",
        "            # extraire les phrases-clés\n",
        "            kp = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "            for k in kp:\n",
        "                print(k)\n",
        "                output_file.write(str(k) + '\\n')\n",
        "        except ValueError as e:\n",
        "            print(f\"An error occurred while processing chunks starting at line {start}: {e}\")\n",
        "            # Accessoirement, écrire un message ou gérer l'erreur si besoin\n",
        "            # output_file.write(\"Pas de phrases-clés extraites pour cette tranche.\\n\")\n"
      ],
      "metadata": {
        "id": "t0pYX7qF365_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d744263-8ab1-4898-e7e3-d1fcd5ab98e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "('14 planches', 0.8301)\n",
            "('progrès médical', 0.787)\n",
            "('leçons', 0.7721)\n",
            "('oeuvres complètes', 0.7623)\n",
            "('35 figures', 0.7263)\n",
            "('œuvres complè', 0.9246)\n",
            "('réimpression', 0.9131)\n",
            "('pachyméningite spinale', 0.9117)\n",
            "('preuve convaincante', 0.8918)\n",
            "('édition précédente', 0.89)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "053ahNazn7jx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}