{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljpetkovic/Charcot_KeyBERT_Keyphrase-Vectorizers/blob/main/scripts/KeyBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Extraction des mots/phrases-clés avec `keybert` et `keyphrase-vectorizers`\n",
        "### Approche _PatternRank_\n",
        "###### [Schopf _et al._, 2022](https://arxiv.org/pdf/2210.05245.pdf)\n",
        "---"
      ],
      "metadata": {
        "id": "9tWVEeLgzpck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJz6QJUR4upy"
      },
      "source": [
        "#1️⃣ `keybert`\n",
        "* _cf._ [Grootendorst (2020)](https://doi.org/10.5281/zenodo.4461265)\n",
        "* librairie Python pour extraire des mots/phrases-clés les plus similaires à un document en exploitant les plongements BERT<br>\n",
        "⚠️ on doit spécifier la longueur des n-grammes à extraire, alors que l'on ne sait pas quelle est la longueur optimale ;<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`keyphrase_ngram_range=(1, 3)` : on veut extraire trois types de séquences : uni-, bi- ou trigrammes <br>\n",
        "⚠️ la grammaticalité des phrases n'est pas prise en compte (p. ex. « scientifique les planches »)\n",
        "\n",
        "**_Maximal Marginal Relevance_**\n",
        "\n",
        "* Afin de diversifier les résultats de l'extraction des mots / phrases-clés, on peut utiliser _Maximal Margin Relevance_ (_MMR_), paramètre également basé sur la similarité cosinus :\n",
        " * `use_mmr=True, diversity=[0-1]` (le degré de diversité entre 0 et 1)\n",
        "\n",
        "\n",
        "\n",
        " **Mots vides**\n",
        "\n",
        " Les listes de mots vides proviennent du vectorizer utilisé avec KeyBERT, et non pas de KeyBERT en soi.\n",
        "\n",
        " * `stop_words=None` : si aucune liste ne s'applique\n",
        " * `stop_words='french'` : si l'on applique une liste de mots vides en français"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "import torch\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keybert import KeyBERT\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize the Sentence Transformer Model\n",
        "sentence_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
        "kw_model = KeyBERT(model=sentence_model)\n",
        "\n",
        "# Download and set up French stop words\n",
        "## si spaCy\n",
        "\n",
        "# Load spaCy French model\n",
        "!python -m spacy download fr_core_news_lg\n",
        "nlp = spacy.load('fr_core_news_lg')\n",
        "# Convert spaCy's set of stop words to a list\n",
        "french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "\n",
        "## si NLTK\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# french_stop_words = stopwords.words('french')\n",
        "\n",
        "# Initialize CountVectorizer with French stop words\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=french_stop_words)\n",
        "\n",
        "# Assuming Google Drive is mounted and paths are correctly set up\n",
        "path = '/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/corpus/'\n",
        "file_name = 'echantillon_charcot.txt'\n",
        "file_path = '../output/keybert_charcot_output.txt'\n",
        "\n",
        "# Function to sort keywords\n",
        "def sort_keywords_by_score(keywords):\n",
        "    # Sort keywords based on the score in descending order\n",
        "    return sorted(keywords, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# List to store all keywords\n",
        "all_keywords = []\n",
        "\n",
        "# Extract keywords from the file\n",
        "with open(os.path.join(path, file_name), 'r') as myfile:\n",
        "    raw_data = myfile.readlines()\n",
        "    start = 0\n",
        "    end = 20\n",
        "    while len(raw_data) >= end:\n",
        "        data = \" \".join(raw_data[start:end])\n",
        "        start = end\n",
        "        end += 20\n",
        "        keywords = kw_model.extract_keywords(data, vectorizer=vectorizer, use_mmr=True, diversity=0.7)\n",
        "        all_keywords.extend(keywords)\n",
        "\n",
        "# Sort all keywords once after extraction\n",
        "sorted_keywords = sort_keywords_by_score(all_keywords)\n",
        "\n",
        "# Write sorted keywords to the output file\n",
        "with open(os.path.join(path, file_path), 'w') as outfile:\n",
        "    for keyword, score in sorted_keywords:\n",
        "        print(f\"{keyword}: {score}\")\n",
        "        outfile.write(f\"{keyword}: {score}\\n\")\n"
      ],
      "metadata": {
        "id": "x6yt0M8kVXAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c12460a5-d23c-4652-badc-6a6f7a30e3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, keybert\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed keybert-0.9.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-497ac0fdae2d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Monter le Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initialize the Sentence Transformer Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2️⃣ PatternRank\n",
        "* `keybert` + **`keyphrase-vectorizers`** = PatternRank<br>\n",
        " ❇️ pas besoin de spécifier la longueur des n-grammes à extraire, car la librairie l'infère elle-même<br>\n",
        "❇️ la grammaticalité des phrases est prise en compte grâce aux extractions des parties du discours (p. ex. `<N.*>*<ADJ.*>*<ADJ.*>+`--> _sclérose latérale amyotrophique_)\n",
        "* _cf._ [Schopf _et al._ (2022)](https://arxiv.org/pdf/2210.05245.pdf) et [Schopf (2022)](https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhJMTOxTfUnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script optimisé en mémoire\n",
        "Date de dernière modification : 07/03/2025"
      ],
      "metadata": {
        "id": "F8JvPf04oCGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keyphrase-vectorizers\n",
        "!pip install keybert\n",
        "!pip install flair\n",
        "!pip install spacy\n",
        "!python -m spacy download fr_core_news_lg\n",
        "\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "\n",
        "# Convert spaCy's stop words to a list\n",
        "french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# Set input/output paths\n",
        "input_path = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/\"\n",
        "output_file_name = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/ALL_KEYPHRASES_Charcot_apartir_CL_000001_005_texte.csv\"\n",
        "\n",
        "# Use CamemBERT model (best for French)\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings(\"camembert-base\"))\n",
        "\n",
        "# Setup vectorizer with a well-formed pattern\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern=(\n",
        "        \"<N.*><ADJ.*>*|\"  # NOUN + optional ADJECTIVE(s)\n",
        "        \"<N.*><P.*><N.*><ADJ.*>*\"  # NOUN + PREPOSITION + NOUN + optional ADJECTIVE(s)\n",
        "    ),\n",
        "    stop_words=french_stop_words\n",
        ")\n",
        "\n",
        "# Get all .txt files in the input directory\n",
        "input_files = glob.glob(os.path.join(input_path, \"*.txt\"))\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(output_file_name, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=\";\")\n",
        "    csv_writer.writerow([\"Filename\", \"Keyphrase\", \"Score\"])  # Header row\n",
        "\n",
        "    # Process each file in the directory\n",
        "    for input_file_name in input_files:\n",
        "        print(f\"Processing file: {input_file_name}\")\n",
        "\n",
        "        with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
        "            buffer = []  # Temporary storage for processing\n",
        "            line_count = 0  # Track lines processed\n",
        "\n",
        "            for line in input_file:\n",
        "                if line.strip():  # Avoid empty lines\n",
        "                    buffer.append(line.strip())  # Store line in buffer\n",
        "                    line_count += 1\n",
        "\n",
        "                if line_count % 500 == 0 and buffer:  # Process every 500 lines\n",
        "                    data = \" \".join(buffer)  # Convert buffer to a string\n",
        "                    buffer = []  # Clear buffer after processing\n",
        "\n",
        "                    try:\n",
        "                        keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                        if keyphrases:  # Ensure we have extracted phrases\n",
        "                            for phrase, score in keyphrases:\n",
        "                                csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Error processing {input_file_name} at line {line_count}: {e}\")\n",
        "\n",
        "            # Process any remaining lines in the buffer\n",
        "            if buffer:\n",
        "                data = \" \".join(buffer)\n",
        "                try:\n",
        "                    keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                    if keyphrases:  # Ensure we have extracted phrases\n",
        "                        for phrase, score in keyphrases:\n",
        "                            csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error processing {input_file_name} at end of file: {e}\")\n",
        "\n",
        "print(f\"✅ Keyphrases saved to {output_file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-ZYGUxgn6jW",
        "outputId": "811bf180-4466-4513-e3fa-c1fbae7042c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keyphrase-vectorizers in /usr/local/lib/python3.11/dist-packages (0.0.13)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (1.26.4)\n",
            "Requirement already satisfied: spacy>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (3.7.5)\n",
            "Requirement already satisfied: spacy-transformers>=1.1.6 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (1.3.8)\n",
            "Requirement already satisfied: spacy-curated-transformers>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (0.3.0)\n",
            "Requirement already satisfied: nltk>=3.6.1 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (3.9.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (1.13.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from keyphrase-vectorizers) (5.9.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.1->keyphrase-vectorizers) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.1->keyphrase-vectorizers) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.1->keyphrase-vectorizers) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.1->keyphrase-vectorizers) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->keyphrase-vectorizers) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (0.15.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->keyphrase-vectorizers) (3.5.0)\n",
            "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (0.1.1)\n",
            "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers<4.50.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers>=1.1.6->keyphrase-vectorizers) (4.48.3)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers>=1.1.6->keyphrase-vectorizers) (0.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->keyphrase-vectorizers) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->keyphrase-vectorizers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->keyphrase-vectorizers) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->keyphrase-vectorizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase-vectorizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase-vectorizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase-vectorizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase-vectorizers) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0.1->keyphrase-vectorizers) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0.1->keyphrase-vectorizers) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers>=0.2.2->keyphrase-vectorizers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase-vectorizers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase-vectorizers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase-vectorizers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase-vectorizers) (0.5.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->keyphrase-vectorizers) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->keyphrase-vectorizers) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->keyphrase-vectorizers) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->keyphrase-vectorizers) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.0.1->keyphrase-vectorizers) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->keyphrase-vectorizers) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->keyphrase-vectorizers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->keyphrase-vectorizers) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.1->keyphrase-vectorizers) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->keyphrase-vectorizers) (0.1.2)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.11/dist-packages (from flair) (1.37.9)\n",
            "Requirement already satisfied: conllu<5.0.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair) (1.2.18)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from flair) (6.3.1)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.28.1)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.11/dist-packages (from flair) (10.6.0)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.11/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from flair) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from flair) (1.6.1)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.11/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from flair) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.67.1)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.48.3)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from flair) (0.8.1)\n",
            "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1)\n",
            "Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (4.0.0)\n",
            "Requirement already satisfied: intervaltree in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (0.6.2)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.9 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.37.9)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (0.11.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mpld3>=0.3->flair) (3.1.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (3.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.25.6)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.9->boto3>=1.20.27->flair) (2.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.1.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting fr-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.7.0/fr_core_news_lg-3.7.0-py3-none-any.whl (571.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from fr-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-lg==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000001_005_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000001_006_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000001_007_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000001_008_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000001_009_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000004_001_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000005_001_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000005_002_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CL_000009_002_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000029_001_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000030_002_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000031_003_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000032_004_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000033_005_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000034_006_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000035_007_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000036_008_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000037_009_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000038_010_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000039_011_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000040_012_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000041_013_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000042_014_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000043_015_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000044_016_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000045_017_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000046_018_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000047_019_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000048_020_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000049_021_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000050_022_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000051_023_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000052_024_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000053_025_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CN_000054_026_texte.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keyphrase-vectorizers\n",
        "#!pip install keybert\n",
        "#!pip install flair\n",
        "#!pip install spacy\n",
        "#!python -m spacy download fr_core_news_lg\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "\n",
        "# Convert spaCy's stop words to a list\n",
        "french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# Set paths\n",
        "path = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Autres/\"\n",
        "output_file_name = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/CA_000003_001_texte_500.csv\"\n",
        "\n",
        "# Use camembert model\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings(\"camembert-base\"))\n",
        "\n",
        "# Setup vectorizer with a well-formed pattern\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern=(\n",
        "        \"<N.*><ADJ.*>*|\"  # NOUN + optional ADJECTIVE(s)\n",
        "        \"<N.*><P.*><N.*><ADJ.*>*\"  # NOUN + PREPOSITION + NOUN + optional ADJECTIVE(s)\n",
        "    ),\n",
        "    stop_words=french_stop_words\n",
        ")\n",
        "\n",
        "# Read and process the file efficiently\n",
        "input_file_name = \"CA_000003_001_texte.txt\"\n",
        "full_input_path = os.path.join(path, input_file_name)\n",
        "\n",
        "if not os.path.exists(full_input_path):\n",
        "    raise FileNotFoundError(f\"❌ ERROR: File not found - {full_input_path}\")\n",
        "\n",
        "with open(full_input_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "    raw_data = input_file.readlines()\n",
        "\n",
        "# Write to file while processing instead of keeping everything in memory\n",
        "with open(os.path.join(path, output_file_name), \"w\", encoding=\"utf-8\") as output_file:\n",
        "    for start in range(0, len(raw_data), 500):  # Process 500 lines at a time\n",
        "        data = \" \".join(raw_data[start:start+500]).replace(\"\\n\", \" \")\n",
        "        try:\n",
        "            keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "            for phrase, score in keyphrases:\n",
        "                output_file.write(f\"{phrase}; {score:.4f}\\n\")\n",
        "        except ValueError as e:\n",
        "            print(f\"An error occurred at chunk starting at line {start}: {e}\")\n",
        "\n",
        "print(f\"✅ Keyphrases saved to {output_file_name}\")\n"
      ],
      "metadata": {
        "id": "OnIVathuyITs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d521c15-8c15-499d-e11f-b69ccea1dcdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Keyphrases saved to /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/CA_000003_001_texte_500.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keyphrase-vectorizers\n",
        "#!pip install keybert\n",
        "#!pip install flair\n",
        "#!pip install spacy\n",
        "#!python -m spacy download fr_core_news_lg\n",
        "\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "\n",
        "# Convert spaCy's stop words to a list\n",
        "french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# Set input/output paths\n",
        "input_path = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Autres/\"\n",
        "output_file_name = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/ALL_KEYPHRASES_apartir_CP_000001_001_texte.txt.csv\"\n",
        "\n",
        "# Use CamemBERT model (best for French)\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings(\"camembert-base\"))\n",
        "\n",
        "# Setup vectorizer with a well-formed pattern\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern=(\n",
        "        \"<N.*><ADJ.*>*|\"  # NOUN + optional ADJECTIVE(s)\n",
        "        \"<N.*><P.*><N.*><ADJ.*>*\"  # NOUN + PREPOSITION + NOUN + optional ADJECTIVE(s)\n",
        "    ),\n",
        "    stop_words=french_stop_words\n",
        ")\n",
        "\n",
        "# Get all .txt files in the input directory\n",
        "input_files = glob.glob(os.path.join(input_path, \"*.txt\"))\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(output_file_name, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=\";\")\n",
        "    csv_writer.writerow([\"Filename\", \"Keyphrase\", \"Score\"])  # Header row\n",
        "\n",
        "    # Process each file in the directory\n",
        "    for input_file_name in input_files:\n",
        "        print(f\"Processing file: {input_file_name}\")\n",
        "\n",
        "        with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
        "            buffer = []  # Temporary storage for processing\n",
        "            line_count = 0  # Track lines processed\n",
        "\n",
        "            for line in input_file:\n",
        "                if line.strip():  # Avoid empty lines\n",
        "                    buffer.append(line.strip())  # Store line in buffer\n",
        "                    line_count += 1\n",
        "\n",
        "                if line_count % 500 == 0 and buffer:  # Process every 500 lines\n",
        "                    data = \" \".join(buffer)  # Convert buffer to a string\n",
        "                    buffer = []  # Clear buffer after processing\n",
        "\n",
        "                    try:\n",
        "                        keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                        if keyphrases:  # Ensure we have extracted phrases\n",
        "                            for phrase, score in keyphrases:\n",
        "                                csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Error processing {input_file_name} at line {line_count}: {e}\")\n",
        "\n",
        "            # Process any remaining lines in the buffer\n",
        "            if buffer:\n",
        "                data = \" \".join(buffer)\n",
        "                try:\n",
        "                    keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                    if keyphrases:  # Ensure we have extracted phrases\n",
        "                        for phrase, score in keyphrases:\n",
        "                            csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error processing {input_file_name} at end of file: {e}\")\n",
        "\n",
        "print(f\"✅ Keyphrases saved to {output_file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IraNEuYY4Wbl",
        "outputId": "1539d031-70ab-4777-9412-7f07b10574dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Autres/CP_000001_001_texte.txt\n",
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Autres/CP_000002_001_texte.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📡 Repérage des phrases-clés communes"
      ],
      "metadata": {
        "id": "WwHXeM_7LgBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "import re\n",
        "import numpy as np\n",
        "pattern = re.compile(r\":.*\\n\")\n",
        "charcot_pr = \"/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/output/charcot_output.txt\"\n",
        "autres_pr = \"/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/output/autres_output.txt\"\n",
        "\n",
        "\n",
        "\n",
        "# N'extraire que des phrases-clés, sans leurs scores\n",
        "with open(charcot_pr, 'r') as input_file_charcot, open(autres_pr, 'r') as input_file_autres:\n",
        "    raw_data_charcot = input_file_charcot.readlines()\n",
        "    raw_data_autres = input_file_autres.readlines()\n",
        "    res_charcot = [pattern.sub(\"\", match) for match in raw_data_charcot]\n",
        "    res_autres = [pattern.sub(\"\", match) for match in raw_data_autres]\n",
        "    # for r in res_charcot:\n",
        "      # print(r)\n",
        "    # for r2 in res_autres:\n",
        "    #   print(r2)\n",
        "\n",
        "    common_elements = np.intersect1d(res_charcot, res_autres)\n",
        "    celem_list = common_elements.tolist()\n",
        "    for c in celem_list:\n",
        "      print(c)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-TPT1MF79Qd",
        "outputId": "20cf6474-9b12-423d-da1e-821ccd7849f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foie\n",
            "hypnotisme\n",
            "planche vii\n",
            "planche xvi\n",
            "planche xxi\n",
            "région lombaire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rSd3fyVd0u07",
        "outputId": "5b599a8d-5cfb-4087-bacf-6722a563fb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.11/dist-packages (from yake) (8.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from yake) (1.26.4)\n",
            "Collecting segtok (from yake)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from yake) (3.4.2)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (from yake) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from segtok->yake) (2024.11.6)\n",
            "Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: segtok, yake\n",
            "Successfully installed segtok-1.5.11 yake-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from yake import KeywordExtractor\n",
        "\n",
        "# Fichier à analyser\n",
        "file_path = \"/content/oai_persee_article_noroi_0029-182x_1955_num_6_1_1077.txt\"\n",
        "\n",
        "# Configuration de YAKE pour le français\n",
        "extractor = KeywordExtractor(lan=\"fr\", top=10)\n",
        "\n",
        "# Lecture du fichier\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Nettoyage du texte\n",
        "text = re.sub(r'\\s+', ' ', text)  # Suppression des espaces multiples\n",
        "text = re.sub(r'\\[.*?\\]', '', text)  # Suppression des références\n",
        "\n",
        "# Extraction des mots-clés\n",
        "keywords = extractor.extract_keywords(text)\n",
        "\n",
        "# Formatage des résultats\n",
        "kw_list = [kw[0].lower() for kw in keywords if len(kw[0]) > 3]  # Filtre les termes courts\n",
        "\n",
        "print(\"Termes clés extraits :\")\n",
        "for i, term in enumerate(set(kw_list)):  # Élimination des doublons\n",
        "    print(f\"{i+1}. {term}\")"
      ],
      "metadata": {
        "id": "k4gfJWOzQasU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bd241c-d0f4-495c-c021-72310842bc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Termes clés extraits :\n",
            "1. zone\n",
            "2. andré guilcher\n",
            "3. bloc\n",
            "4. roche\n",
            "5. trégor finistérien\n",
            "6. mécanique\n",
            "7. action mécanique\n",
            "8. destable\n",
            "9. guilcher\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keyphrase-vectorizers keybert flair spacy\n",
        "!python -m spacy download fr_core_news_lg\n",
        "\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from keybert import KeyBERT\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "# Charger les modèles\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings('camembert-base'))\n",
        "\n",
        "# Configuration du vectorizer\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern='<N.+>+<ADJ.*>*<PREP>?<N.+>*',\n",
        "    stop_words=list(nlp.Defaults.stop_words)\n",
        ")\n",
        "\n",
        "# Chemins\n",
        "drive_path = '/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/corpus/'\n",
        "input_dir = os.path.join(drive_path, '2000s')  # Dossier contenant les 62 fichiers\n",
        "output_dir = os.path.join(drive_path, 'output_2000s')  # Dossier de sortie\n",
        "\n",
        "# Créer le dossier de sortie s'il n'existe pas\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Traiter tous les fichiers .txt du dossier\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(input_dir, filename)\n",
        "\n",
        "        try:\n",
        "            # Lire le fichier\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().replace('\\n', ' ')\n",
        "\n",
        "            # Lemmatisation\n",
        "            doc = nlp(text)\n",
        "            lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "            # Extraction de mots-clés\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                lemmatized_text,\n",
        "                vectorizer=vectorizer,\n",
        "                keyphrase_ngram_range=(1, 3),\n",
        "                use_mmr=True,\n",
        "                diversity=0.7,\n",
        "                top_n=50\n",
        "            )\n",
        "\n",
        "            # Fusion des doublons\n",
        "            keyword_scores = defaultdict(float)\n",
        "            for kw, score in keywords:\n",
        "                keyword_scores[kw] += score\n",
        "\n",
        "            # Tri par score\n",
        "            sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Enregistrer les résultats\n",
        "            output_path = os.path.join(output_dir, f'keywords_{filename}')\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                for kw, score in sorted_keywords:\n",
        "                    f.write(f\"{kw}: {score}\\n\")\n",
        "\n",
        "            print(f\"Traitement réussi : {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec le fichier {filename} : {str(e)}\")\n",
        "            continue\n",
        "\n",
        "print(\"Traitement de tous les fichiers terminé!\")"
      ],
      "metadata": {
        "id": "bYrWvFgEVhK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}