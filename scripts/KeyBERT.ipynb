{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljpetkovic/Charcot_KeyBERT_Keyphrase-Vectorizers/blob/main/scripts/KeyBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction des mots/phrases-clés avec `keybert` et `keyphrase-vectorizers`\n",
        "### Approche _PatternRank_\n",
        "###### [Schopf _et al._, 2022](https://arxiv.org/pdf/2210.05245.pdf)\n",
        "---"
      ],
      "metadata": {
        "id": "9tWVEeLgzpck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJz6QJUR4upy"
      },
      "source": [
        "#1️⃣ `keybert`\n",
        "* _cf._ [Grootendorst (2020)](https://doi.org/10.5281/zenodo.4461265)\n",
        "* librairie Python pour extraire des mots/phrases-clés les plus similaires à un document en exploitant les plongements BERT<br>\n",
        "⚠️ on doit spécifier la longueur des n-grammes à extraire, alors que l'on ne sait pas quelle est la longueur optimale ;<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`keyphrase_ngram_range=(1, 3)` : on veut extraire trois types de séquences : uni-, bi- ou trigrammes <br>\n",
        "⚠️ la grammaticalité des phrases n'est pas prise en compte (p. ex. « scientifique les planches »)\n",
        "\n",
        "**_Maximal Marginal Relevance_**\n",
        "\n",
        "* Afin de diversifier les résultats de l'extraction des mots / phrases-clés, on peut utiliser _Maximal Margin Relevance_ (_MMR_), paramètre également basé sur la similarité cosinus :\n",
        " * `use_mmr=True, diversity=[0-1]` (le degré de diversité entre 0 et 1)\n",
        "\n",
        "\n",
        "\n",
        " **Mots vides**\n",
        "\n",
        " Les listes de mots vides proviennent du vectorizer utilisé avec KeyBERT, et non pas de KeyBERT en soi.\n",
        "\n",
        " * `stop_words=None` : si aucune liste ne s'applique\n",
        " * `stop_words='french'` : si l'on applique une liste de mots vides en français"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "import torch\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keybert import KeyBERT\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize the Sentence Transformer Model\n",
        "sentence_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
        "kw_model = KeyBERT(model=sentence_model)\n",
        "\n",
        "# Download and set up French stop words\n",
        "## si spaCy\n",
        "\n",
        "# Load spaCy French model\n",
        "!python -m spacy download fr_core_news_lg\n",
        "nlp = spacy.load('fr_core_news_lg')\n",
        "# Convert spaCy's set of stop words to a list\n",
        "french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "\n",
        "## si NLTK\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# french_stop_words = stopwords.words('french')\n",
        "\n",
        "# Initialize CountVectorizer with French stop words\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=french_stop_words)\n",
        "\n",
        "# Assuming Google Drive is mounted and paths are correctly set up\n",
        "path = '/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/corpus/'\n",
        "file_name = 'echantillon_charcot.txt'\n",
        "file_path = '../output/keybert_charcot_output.txt'\n",
        "\n",
        "# Function to sort keywords\n",
        "def sort_keywords_by_score(keywords):\n",
        "    # Sort keywords based on the score in descending order\n",
        "    return sorted(keywords, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# List to store all keywords\n",
        "all_keywords = []\n",
        "\n",
        "# Extract keywords from the file\n",
        "with open(os.path.join(path, file_name), 'r') as myfile:\n",
        "    raw_data = myfile.readlines()\n",
        "    start = 0\n",
        "    end = 20\n",
        "    while len(raw_data) >= end:\n",
        "        data = \" \".join(raw_data[start:end])\n",
        "        start = end\n",
        "        end += 20\n",
        "        keywords = kw_model.extract_keywords(data, vectorizer=vectorizer, use_mmr=True, diversity=0.7)\n",
        "        all_keywords.extend(keywords)\n",
        "\n",
        "# Sort all keywords once after extraction\n",
        "sorted_keywords = sort_keywords_by_score(all_keywords)\n",
        "\n",
        "# Write sorted keywords to the output file\n",
        "with open(os.path.join(path, file_path), 'w') as outfile:\n",
        "    for keyword, score in sorted_keywords:\n",
        "        print(f\"{keyword}: {score}\")\n",
        "        outfile.write(f\"{keyword}: {score}\\n\")\n"
      ],
      "metadata": {
        "id": "x6yt0M8kVXAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2️⃣ PatternRank\n",
        "* `keybert` + **`keyphrase-vectorizers`** = PatternRank<br>\n",
        " ❇️ pas besoin de spécifier la longueur des n-grammes à extraire, car la librairie l'infère elle-même<br>\n",
        "❇️ la grammaticalité des phrases est prise en compte grâce aux extractions des parties du discours (p. ex. `<N.*>*<ADJ.*>*<ADJ.*>+`--> _sclérose latérale amyotrophique_)\n",
        "* _cf._ [Schopf _et al._ (2022)](https://arxiv.org/pdf/2210.05245.pdf) et [Schopf (2022)](https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhJMTOxTfUnx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCUTu0qSoBaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script optimisé en mémoire et sans utiliser des mots vides\n",
        "Date de dernière modification : 31/07/2025"
      ],
      "metadata": {
        "id": "F8JvPf04oCGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keyphrase-vectorizers\n",
        "#!pip install keybert\n",
        "#!pip install flair\n",
        "#!pip install spacy\n",
        "#!python -m spacy download fr_core_news_lg\n",
        "\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "\n",
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "\n",
        "# Convert spaCy's stop words to a list\n",
        "# french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# Set input/output paths\n",
        "input_path = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/\"\n",
        "output_file_name = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/charcot_020825/CO_000092_010_texte.csv\"\n",
        "\n",
        "# Use CamemBERT model (best for French)\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings(\"camembert-base\"))\n",
        "\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern=(\n",
        "       # \"<NOUN>+\"                                 # NOM ou plusieurs NOM\n",
        "        \"<NOUN>+<NOUN>+\"                         # NOM + NOM\n",
        "        \"|<NOUN>+<ADJ>+\"                          # NOM + ADJ\n",
        "        \"|<NOUN>+<ADJ>+<ADJ>*\"                    # NOM + plusieurs ADJ\n",
        "        \"|<NOUN>+<ADP><NOUN>+\"                    # NOM + PRÉP + NOM\n",
        "        \"|<NOUN>+<ADP><NOUN>+<ADJ>*\"              # NOM + PRÉP + NOM + (ADJ*)\n",
        "        \"|<NOUN>+<ADP><NOUN>+<ADJ>+<ADJ>*\"        # NOM + PRÉP + NOM + plusieurs ADJ\n",
        "    )\n",
        "    # stop_words=french_stop_words\n",
        ")\n",
        "\n",
        "\n",
        "# Get all .txt files in the input directory\n",
        "input_files = glob.glob(os.path.join(input_path, \"CO_000092_010_texte.txt\"))\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(output_file_name, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=\";\")\n",
        "    csv_writer.writerow([\"Filename\", \"Keyphrase\", \"Score\"])  # Header row\n",
        "\n",
        "    # Process each file in the directory\n",
        "    for input_file_name in input_files:\n",
        "        print(f\"Processing file: {input_file_name}\")\n",
        "\n",
        "        with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
        "            buffer = []  # Temporary storage for processing\n",
        "            line_count = 0  # Track lines processed\n",
        "\n",
        "            for line in input_file:\n",
        "                if line.strip():  # Avoid empty lines\n",
        "                    buffer.append(line.strip())  # Store line in buffer\n",
        "                    line_count += 1\n",
        "\n",
        "                if line_count % 500 == 0 and buffer:  # Process every 500 lines\n",
        "                    data = \" \".join(buffer)  # Convert buffer to a string\n",
        "                    buffer = []  # Clear buffer after processing\n",
        "\n",
        "                    try:\n",
        "                        keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                        if keyphrases:  # Ensure we have extracted phrases\n",
        "                            for phrase, score in keyphrases:\n",
        "                                csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Error processing {input_file_name} at line {line_count}: {e}\")\n",
        "\n",
        "            # Process any remaining lines in the buffer\n",
        "            if buffer:\n",
        "                data = \" \".join(buffer)\n",
        "                try:\n",
        "                    keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                    if keyphrases:  # Ensure we have extracted phrases\n",
        "                        for phrase, score in keyphrases:\n",
        "                            csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error processing {input_file_name} at end of file: {e}\")\n",
        "\n",
        "print(f\"✅ Keyphrases saved to {output_file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-ZYGUxgn6jW",
        "outputId": "81703ce8-6a67-461c-ff9b-77ac62d41864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Charcot/CO_000092_010_texte.txt\n",
            "✅ Keyphrases saved to /content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/charcot_020825/CO_000092_010_texte.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9-z1LPG23Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bloopers"
      ],
      "metadata": {
        "id": "lkW__uaR24WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keyphrase-vectorizers\n",
        "#!pip install keybert\n",
        "#!pip install flair\n",
        "#!pip install spacy\n",
        "#!python -m spacy download fr_core_news_lg\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "\n",
        "# Convert spaCy's stop words to a list\n",
        "french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# Set paths\n",
        "path = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Autres/\"\n",
        "output_file_name = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/CA_000003_001_texte_500.csv\"\n",
        "\n",
        "# Use camembert model\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings(\"camembert-base\"))\n",
        "\n",
        "# Setup vectorizer with a well-formed pattern\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern=(\n",
        "        \"<N.*><ADJ.*>*|\"  # NOUN + optional ADJECTIVE(s)\n",
        "        \"<N.*><P.*><N.*><ADJ.*>*\"  # NOUN + PREPOSITION + NOUN + optional ADJECTIVE(s)\n",
        "    ),\n",
        "    stop_words=french_stop_words\n",
        ")\n",
        "\n",
        "# Read and process the file efficiently\n",
        "input_file_name = \"CA_000003_001_texte.txt\"\n",
        "full_input_path = os.path.join(path, input_file_name)\n",
        "\n",
        "if not os.path.exists(full_input_path):\n",
        "    raise FileNotFoundError(f\"❌ ERROR: File not found - {full_input_path}\")\n",
        "\n",
        "with open(full_input_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "    raw_data = input_file.readlines()\n",
        "\n",
        "# Write to file while processing instead of keeping everything in memory\n",
        "with open(os.path.join(path, output_file_name), \"w\", encoding=\"utf-8\") as output_file:\n",
        "    for start in range(0, len(raw_data), 500):  # Process 500 lines at a time\n",
        "        data = \" \".join(raw_data[start:start+500]).replace(\"\\n\", \" \")\n",
        "        try:\n",
        "            keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "            for phrase, score in keyphrases:\n",
        "                output_file.write(f\"{phrase}; {score:.4f}\\n\")\n",
        "        except ValueError as e:\n",
        "            print(f\"An error occurred at chunk starting at line {start}: {e}\")\n",
        "\n",
        "print(f\"✅ Keyphrases saved to {output_file_name}\")\n"
      ],
      "metadata": {
        "id": "OnIVathuyITs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keyphrase-vectorizers\n",
        "#!pip install keybert\n",
        "#!pip install flair\n",
        "#!pip install spacy\n",
        "#!python -m spacy download fr_core_news_lg\n",
        "\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "# Monter le Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "\n",
        "# Convert spaCy's stop words to a list\n",
        "french_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# Set input/output paths\n",
        "input_path = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/corpus/txt_corpus_Autres/\"\n",
        "output_file_name = \"/content/drive/MyDrive/ObTIC/Charcot/Keyphrase-Vectorizers/output/ALL_KEYPHRASES_apartir_CP_000001_001_texte.txt.csv\"\n",
        "\n",
        "# Use CamemBERT model (best for French)\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings(\"camembert-base\"))\n",
        "\n",
        "# Setup vectorizer with a well-formed pattern\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern=(\n",
        "        \"<N.*><ADJ.*>*|\"  # NOUN + optional ADJECTIVE(s)\n",
        "        \"<N.*><P.*><N.*><ADJ.*>*\"  # NOUN + PREPOSITION + NOUN + optional ADJECTIVE(s)\n",
        "    ),\n",
        "    stop_words=french_stop_words\n",
        ")\n",
        "\n",
        "# Get all .txt files in the input directory\n",
        "input_files = glob.glob(os.path.join(input_path, \"*.txt\"))\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(output_file_name, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=\";\")\n",
        "    csv_writer.writerow([\"Filename\", \"Keyphrase\", \"Score\"])  # Header row\n",
        "\n",
        "    # Process each file in the directory\n",
        "    for input_file_name in input_files:\n",
        "        print(f\"Processing file: {input_file_name}\")\n",
        "\n",
        "        with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
        "            buffer = []  # Temporary storage for processing\n",
        "            line_count = 0  # Track lines processed\n",
        "\n",
        "            for line in input_file:\n",
        "                if line.strip():  # Avoid empty lines\n",
        "                    buffer.append(line.strip())  # Store line in buffer\n",
        "                    line_count += 1\n",
        "\n",
        "                if line_count % 500 == 0 and buffer:  # Process every 500 lines\n",
        "                    data = \" \".join(buffer)  # Convert buffer to a string\n",
        "                    buffer = []  # Clear buffer after processing\n",
        "\n",
        "                    try:\n",
        "                        keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                        if keyphrases:  # Ensure we have extracted phrases\n",
        "                            for phrase, score in keyphrases:\n",
        "                                csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Error processing {input_file_name} at line {line_count}: {e}\")\n",
        "\n",
        "            # Process any remaining lines in the buffer\n",
        "            if buffer:\n",
        "                data = \" \".join(buffer)\n",
        "                try:\n",
        "                    keyphrases = kw_model.extract_keywords(data, vectorizer=vectorizer)\n",
        "                    if keyphrases:  # Ensure we have extracted phrases\n",
        "                        for phrase, score in keyphrases:\n",
        "                            csv_writer.writerow([os.path.basename(input_file_name), phrase, f\"{score:.4f}\"])\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error processing {input_file_name} at end of file: {e}\")\n",
        "\n",
        "print(f\"✅ Keyphrases saved to {output_file_name}\")\n"
      ],
      "metadata": {
        "id": "IraNEuYY4Wbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📡 Repérage des phrases-clés communes"
      ],
      "metadata": {
        "id": "WwHXeM_7LgBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_dataframe(filepath):\n",
        "    for sep in [\",\", \";\", \"\\t\"]:\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, sep=sep)\n",
        "            if \"Keyphrase\" in df.columns and \"Score\" in df.columns:\n",
        "                df = df[df[\"Keyphrase\"] != \"Keyphrase\"]\n",
        "                df[\"Keyphrase\"] = df[\"Keyphrase\"].str.strip().str.lower()\n",
        "                df[\"Score\"] = pd.to_numeric(df[\"Score\"], errors=\"coerce\")\n",
        "                df = df.dropna(subset=[\"Score\"])\n",
        "                return df\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise ValueError(\"Format de fichier inconnu ou séparateur incorrect.\")\n",
        "\n",
        "# Chemins d'entrée\n",
        "charcot_file = \"charcot_kpv_060825.csv\"\n",
        "autres_file = \"autres_echantillon_kpv_060825.csv\"\n",
        "\n",
        "# Chargement des DataFrames\n",
        "charcot_df = load_dataframe(charcot_file)\n",
        "autres_df = load_dataframe(autres_file)\n",
        "\n",
        "# Moyennes des scores par keyphrase\n",
        "charcot_scores = charcot_df.groupby(\"Keyphrase\")[\"Score\"].mean().rename(\"Score_Charcot\")\n",
        "autres_scores = autres_df.groupby(\"Keyphrase\")[\"Score\"].mean().rename(\"Score_Autres\")\n",
        "\n",
        "# Termes en commun\n",
        "common_terms = set(charcot_scores.index) & set(autres_scores.index)\n",
        "\n",
        "# Fusion\n",
        "merged = pd.concat([charcot_scores, autres_scores], axis=1).loc[list(common_terms)]\n",
        "merged[\"Score_Moyen\"] = merged[[\"Score_Charcot\", \"Score_Autres\"]].mean(axis=1)\n",
        "\n",
        "# Arrondi des scores\n",
        "merged = merged.round(3)\n",
        "\n",
        "# Tri décroissant par score moyen\n",
        "merged_sorted = merged.sort_values(by=\"Score_Moyen\", ascending=False).reset_index()\n",
        "\n",
        "# Sauvegarde\n",
        "merged_sorted.to_csv(\"termes_en_commun.csv\", index=False)\n",
        "print(\"Fichier 'termes_en_commun.csv' généré avec scores arrondis à 3 décimales.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPguvkO_ki1m",
        "outputId": "8b944951-ebcc-48ba-e5b5-c65f9ce8966c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier 'termes_en_commun.csv' généré avec scores arrondis à 3 décimales.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "import re\n",
        "import numpy as np\n",
        "pattern = re.compile(r\":.*\\n\")\n",
        "charcot_pr = \"/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/output/charcot_output.txt\"\n",
        "autres_pr = \"/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/output/autres_output.txt\"\n",
        "\n",
        "\n",
        "\n",
        "# N'extraire que des phrases-clés, sans leurs scores\n",
        "with open(charcot_pr, 'r') as input_file_charcot, open(autres_pr, 'r') as input_file_autres:\n",
        "    raw_data_charcot = input_file_charcot.readlines()\n",
        "    raw_data_autres = input_file_autres.readlines()\n",
        "    res_charcot = [pattern.sub(\"\", match) for match in raw_data_charcot]\n",
        "    res_autres = [pattern.sub(\"\", match) for match in raw_data_autres]\n",
        "    # for r in res_charcot:\n",
        "      # print(r)\n",
        "    # for r2 in res_autres:\n",
        "    #   print(r2)\n",
        "\n",
        "    common_elements = np.intersect1d(res_charcot, res_autres)\n",
        "    celem_list = common_elements.tolist()\n",
        "    for c in celem_list:\n",
        "      print(c)\n",
        "\n"
      ],
      "metadata": {
        "id": "d-TPT1MF79Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rSd3fyVd0u07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from yake import KeywordExtractor\n",
        "\n",
        "# Fichier à analyser\n",
        "file_path = \"/content/oai_persee_article_noroi_0029-182x_1955_num_6_1_1077.txt\"\n",
        "\n",
        "# Configuration de YAKE pour le français\n",
        "extractor = KeywordExtractor(lan=\"fr\", top=10)\n",
        "\n",
        "# Lecture du fichier\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Nettoyage du texte\n",
        "text = re.sub(r'\\s+', ' ', text)  # Suppression des espaces multiples\n",
        "text = re.sub(r'\\[.*?\\]', '', text)  # Suppression des références\n",
        "\n",
        "# Extraction des mots-clés\n",
        "keywords = extractor.extract_keywords(text)\n",
        "\n",
        "# Formatage des résultats\n",
        "kw_list = [kw[0].lower() for kw in keywords if len(kw[0]) > 3]  # Filtre les termes courts\n",
        "\n",
        "print(\"Termes clés extraits :\")\n",
        "for i, term in enumerate(set(kw_list)):  # Élimination des doublons\n",
        "    print(f\"{i+1}. {term}\")"
      ],
      "metadata": {
        "id": "k4gfJWOzQasU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keyphrase-vectorizers keybert flair spacy\n",
        "!python -m spacy download fr_core_news_lg\n",
        "\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from keybert import KeyBERT\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "# Charger les modèles\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "kw_model = KeyBERT(model=TransformerDocumentEmbeddings('camembert-base'))\n",
        "\n",
        "# Configuration du vectorizer\n",
        "vectorizer = KeyphraseCountVectorizer(\n",
        "    spacy_pipeline=nlp,\n",
        "    pos_pattern='<N.+>+<ADJ.*>*<PREP>?<N.+>*',\n",
        "    stop_words=list(nlp.Defaults.stop_words)\n",
        ")\n",
        "\n",
        "# Chemins\n",
        "drive_path = '/content/drive/MyDrive/ObTIC/ateliers/extraction_mots_cles/corpus/'\n",
        "input_dir = os.path.join(drive_path, '2000s')  # Dossier contenant les 62 fichiers\n",
        "output_dir = os.path.join(drive_path, 'output_2000s')  # Dossier de sortie\n",
        "\n",
        "# Créer le dossier de sortie s'il n'existe pas\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Traiter tous les fichiers .txt du dossier\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(input_dir, filename)\n",
        "\n",
        "        try:\n",
        "            # Lire le fichier\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().replace('\\n', ' ')\n",
        "\n",
        "            # Lemmatisation\n",
        "            doc = nlp(text)\n",
        "            lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "            # Extraction de mots-clés\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                lemmatized_text,\n",
        "                vectorizer=vectorizer,\n",
        "                keyphrase_ngram_range=(1, 3),\n",
        "                use_mmr=True,\n",
        "                diversity=0.7,\n",
        "                top_n=50\n",
        "            )\n",
        "\n",
        "            # Fusion des doublons\n",
        "            keyword_scores = defaultdict(float)\n",
        "            for kw, score in keywords:\n",
        "                keyword_scores[kw] += score\n",
        "\n",
        "            # Tri par score\n",
        "            sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Enregistrer les résultats\n",
        "            output_path = os.path.join(output_dir, f'keywords_{filename}')\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                for kw, score in sorted_keywords:\n",
        "                    f.write(f\"{kw}: {score}\\n\")\n",
        "\n",
        "            print(f\"Traitement réussi : {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec le fichier {filename} : {str(e)}\")\n",
        "            continue\n",
        "\n",
        "print(\"Traitement de tous les fichiers terminé!\")"
      ],
      "metadata": {
        "id": "bYrWvFgEVhK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}